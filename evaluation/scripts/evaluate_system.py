"""
ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ì‘ë‹µ ì‹œê°„
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ì„±ê³µë¥ 
"""

import json
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import time
import random

# ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "backend"))

import psutil
import numpy as np


def measure_response_time(agent, question: str) -> Dict[str, Any]:
    """ì‘ë‹µ ì‹œê°„ ì¸¡ì •"""
    start_time = time.time()
    success = False
    error_message = None
    response = None

    try:
        response = agent.invoke({
            "input": question,
            "conversation_id": "eval_session",
            "chat_history": []
        })
        success = True
    except Exception as e:
        error_message = str(e)

    end_time = time.time()
    latency = end_time - start_time

    return {
        "latency": latency,
        "success": success,
        "error": error_message,
        "response": response
    }


def get_memory_usage() -> float:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def evaluate_system_performance(
    agent,
    test_data: List[Dict[str, Any]],
    warmup_questions: int = 3,
    sample_size: int = None
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì „ì²´ í‰ê°€"""

    # ìƒ˜í”Œë§ (ë¹ ë¥¸ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´)
    if sample_size and sample_size > 0 and sample_size < len(test_data):
        test_data = random.sample(test_data, sample_size)
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ìƒ˜í”Œ í‰ê°€: {len(test_data)}ê°œ ì§ˆë¬¸ ì‚¬ìš©\n")

    # ì›Œë°ì—… (ì²« ëª‡ ê°œ ì§ˆë¬¸ìœ¼ë¡œ ìºì‹œ/ì´ˆê¸°í™”)
    print(f"ì›Œë°ì—… ì¤‘... ({warmup_questions}ê°œ ì§ˆë¬¸)")
    for i in range(min(warmup_questions, len(test_data))):
        try:
            agent.invoke({
                "input": test_data[i]["question"],
                "conversation_id": "eval_warmup",
                "chat_history": []
            })
        except:
            pass
        time.sleep(0.5)

    latencies = []
    memory_usage = []
    successes = []
    results = []

    initial_memory = get_memory_usage()
    print(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB\n")

    for i, item in enumerate(test_data):
        question = item["question"]
        category = item.get("category", "unknown")

        print(f"[{i+1}/{len(test_data)}] í…ŒìŠ¤íŠ¸ ì¤‘: {question[:30]}...")

        # ë©”ëª¨ë¦¬ ì¸¡ì • (before)
        mem_before = get_memory_usage()

        # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        result = measure_response_time(agent, question)

        # ë©”ëª¨ë¦¬ ì¸¡ì • (after)
        mem_after = get_memory_usage()

        latencies.append(result["latency"])
        memory_usage.append(mem_after)
        successes.append(result["success"])

        results.append({
            "question": question,
            "category": category,
            "latency": result["latency"],
            "memory_before": mem_before,
            "memory_after": mem_after,
            "memory_delta": mem_after - mem_before,
            "success": result["success"],
            "error": result["error"]
        })

        # Rate limiting
        time.sleep(0.3)

    final_memory = get_memory_usage()
    success_rate = sum(successes) / len(successes) if successes else 0

    return {
        "summary": {
            "total_evaluated": len(test_data),
            "latency": {
                "mean": float(np.mean(latencies)),
                "std": float(np.std(latencies)),
                "min": float(np.min(latencies)),
                "max": float(np.max(latencies)),
                "p50": float(np.percentile(latencies, 50)),
                "p90": float(np.percentile(latencies, 90)),
                "p99": float(np.percentile(latencies, 99))
            },
            "memory": {
                "initial_mb": initial_memory,
                "final_mb": final_memory,
                "peak_mb": float(np.max(memory_usage)),
                "mean_mb": float(np.mean(memory_usage)),
                "growth_mb": final_memory - initial_memory
            },
            "success_rate": success_rate,
            "total_successes": sum(successes),
            "total_failures": len(successes) - sum(successes)
        },
        "details": results,
        "by_category": calculate_category_performance(results)
    }


def calculate_category_performance(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ í†µê³„"""
    categories = {}

    for result in results:
        category = result.get("category", "unknown")
        if category not in categories:
            categories[category] = {
                "latencies": [],
                "successes": []
            }

        categories[category]["latencies"].append(result["latency"])
        categories[category]["successes"].append(result["success"])

    stats = {}
    for category, data in categories.items():
        stats[category] = {
            "count": len(data["latencies"]),
            "avg_latency": float(np.mean(data["latencies"])),
            "success_rate": sum(data["successes"]) / len(data["successes"]) if data["successes"] else 0
        }

    return stats


def main():
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰"""
    import argparse
    parser = argparse.ArgumentParser(description="ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€")
    parser.add_argument("--sample", "-s", type=int, help="ìƒ˜í”Œ í¬ê¸° (ì „ì²´ ëŒ€ì‹  ì¼ë¶€ë§Œ)")
    args = parser.parse_args()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    dataset_path = Path(__file__).parent.parent / "datasets" / "test_questions_prompt_pruned.json"

    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    test_questions = data["questions"]

    # Agent ì´ˆê¸°í™”
    try:
        from agent.agent import create_agent
        agent = create_agent()

        results = evaluate_system_performance(agent, test_questions, sample_size=args.sample)

        # ê²°ê³¼ ì €ì¥
        output_path = Path(__file__).parent.parent / "results" / "system_evaluation.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: {output_path}")
        return results

    except ImportError as e:
        print(f"âš ï¸ Agent ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("ë°±ì—”ë“œì˜ agents ëª¨ë“ˆ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return {"error": str(e)}


if __name__ == "__main__":
    results = main()
    if "summary" in results:
        print("\n=== ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìš”ì•½ ===")
        summary = results["summary"]
        print(f"í‰ê°€ ì§ˆë¬¸ ìˆ˜: {summary['total_evaluated']}")
        print(f"\nì‘ë‹µ ì‹œê°„:")
        print(f"  í‰ê· : {summary['latency']['mean']:.2f}s (Â±{summary['latency']['std']:.2f}s)")
        print(f"  P50: {summary['latency']['p50']:.2f}s")
        print(f"  P90: {summary['latency']['p90']:.2f}s")
        print(f"  P99: {summary['latency']['p99']:.2f}s")
        print(f"  ìµœì†Œ/ìµœëŒ€: {summary['latency']['min']:.2f}s / {summary['latency']['max']:.2f}s")
        print(f"\në©”ëª¨ë¦¬:")
        print(f"  ì´ˆê¸°: {summary['memory']['initial_mb']:.1f} MB")
        print(f"  ìµœì¢…: {summary['memory']['final_mb']:.1f} MB")
        print(f"  í”¼í¬: {summary['memory']['peak_mb']:.1f} MB")
        print(f"  ì¦ê°€ëŸ‰: {summary['memory']['growth_mb']:.1f} MB")
        print(f"\nì„±ê³µë¥ : {summary['success_rate']:.1%} ({summary['total_successes']}/{summary['total_evaluated']})")

        if "by_category" in results:
            print("\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
            for category, stats in results["by_category"].items():
                print(f"  {category}: {stats['avg_latency']:.2f}s, ì„±ê³µë¥  {stats['success_rate']:.1%} (n={stats['count']})")
