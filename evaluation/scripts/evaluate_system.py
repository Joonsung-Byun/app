"""
ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- ì‘ë‹µ ì‹œê°„
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- ì„±ê³µë¥ 
"""

import json
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import time
import random

# ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "backend"))

import psutil
import numpy as np
from evaluation.scripts.eval_cases import classify_case
from backend.utils.tool_timings import get_and_reset as get_tool_timings


def measure_response_time(agent, question: str) -> Dict[str, Any]:
    """ì‘ë‹µ ì‹œê°„ ì¸¡ì •"""
    start_time = time.time()
    success = False
    error_message = None
    response = None

    try:
        response = agent.invoke({
            "input": question,
            "conversation_id": "eval_session",
            "chat_history": []
        })
        success = True
    except Exception as e:
        error_message = str(e)

    end_time = time.time()
    latency = end_time - start_time

    return {
        "latency": latency,
        "success": success,
        "error": error_message,
        "response": response
    }


def get_memory_usage() -> float:
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024


def evaluate_system_performance(
    agent,
    test_data: List[Dict[str, Any]],
    warmup_questions: int = 3,
    sample_size: int = None,
    meta: Dict[str, Any] = None,
    case_weights: Dict[str, float] = None
) -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ ì „ì²´ í‰ê°€"""
    meta = meta or {}

    # ìƒ˜í”Œë§ (ë¹ ë¥¸ ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•´)
    if sample_size and sample_size > 0 and sample_size < len(test_data):
        test_data = random.sample(test_data, sample_size)
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥ ìƒ˜í”Œ í‰ê°€: {len(test_data)}ê°œ ì§ˆë¬¸ ì‚¬ìš©\n")

    # íƒ€ì´ë° ê¸°ë¡ ì´ˆê¸°í™”
    get_tool_timings()

    # ì›Œë°ì—… (ì²« ëª‡ ê°œ ì§ˆë¬¸ìœ¼ë¡œ ìºì‹œ/ì´ˆê¸°í™”)
    print(f"ì›Œë°ì—… ì¤‘... ({warmup_questions}ê°œ ì§ˆë¬¸)")
    for i in range(min(warmup_questions, len(test_data))):
        try:
            agent.invoke({
                "input": test_data[i]["question"],
                "conversation_id": "eval_warmup",
                "chat_history": []
            })
        except:
            pass
        time.sleep(0.5)

    latencies = []
    memory_usage = []
    successes = []
    results = []
    by_case_latencies = {}
    conv_case_map = {}

    initial_memory = get_memory_usage()
    print(f"ì´ˆê¸° ë©”ëª¨ë¦¬: {initial_memory:.1f} MB\n")

    for i, item in enumerate(test_data):
        question = item["question"]
        category = item.get("category", "unknown")
        ctype = classify_case(item, meta)
        conv_id = f"eval_system_{i}"
        conv_case_map[conv_id] = ctype

        print(f"[{i+1}/{len(test_data)}] í…ŒìŠ¤íŠ¸ ì¤‘: {question[:30]}...")

        # ë©”ëª¨ë¦¬ ì¸¡ì • (before)
        mem_before = get_memory_usage()

        # ì‘ë‹µ ì‹œê°„ ì¸¡ì •
        start_time = time.time()
        try:
            response = agent.invoke({
                "input": question,
                "conversation_id": conv_id,
                "chat_history": []
            })
            success = True
            error_message = None
        except Exception as e:
            success = False
            error_message = str(e)
        latency = time.time() - start_time
        result = {
            "latency": latency,
            "success": success,
            "error": error_message,
            "response": None
        }

        # ë©”ëª¨ë¦¬ ì¸¡ì • (after)
        mem_after = get_memory_usage()

        latencies.append(result["latency"])
        memory_usage.append(mem_after)
        successes.append(result["success"])

        results.append({
            "question": question,
            "category": category,
            "case": ctype,
            "latency": result["latency"],
            "memory_before": mem_before,
            "memory_after": mem_after,
            "memory_delta": mem_after - mem_before,
            "success": result["success"],
            "error": result["error"]
        })

        # ì¼€ì´ìŠ¤ë³„ ê¸°ë¡
        if ctype not in by_case_latencies:
            by_case_latencies[ctype] = []
        by_case_latencies[ctype].append(result["latency"])

        # Rate limiting
        time.sleep(0.3)

    final_memory = get_memory_usage()
    success_rate = sum(successes) / len(successes) if successes else 0

    # ì¼€ì´ìŠ¤ë³„ í‰ê·  ë° ê°€ì¤‘ í‰ê· 
    by_case_latency = {
        c: {
            "count": len(vals),
            "mean": float(np.mean(vals)),
            "p90": float(np.percentile(vals, 90)),
        }
        for c, vals in by_case_latencies.items()
    }
    weighted_latency = None
    if case_weights:
        total_w = sum(case_weights.values())
        if total_w > 0:
            weighted_latency = sum(
                case_weights.get(c, 0) / total_w * by_case_latency.get(c, {}).get("mean", 0)
                for c in case_weights.keys()
            )

    # íˆ´ë³„ ì‹¤í–‰ ì‹œê°„ (ë°±ì—”ë“œ ì½œë°± ê¸°ë¡ í™œìš©)
    timing_records = get_tool_timings()
    per_tool_time = {}
    per_case_tool_time = {}
    if timing_records:
        from collections import defaultdict
        tmp = defaultdict(list)
        tmp_case = defaultdict(lambda: defaultdict(list))
        for rec in timing_records:
            tool_name = rec.get("tool", "unknown_tool")
            tmp[tool_name].append(rec.get("duration", 0))
            case = conv_case_map.get(rec.get("conversation_id"), "unknown")
            tmp_case[case][tool_name].append(rec.get("duration", 0))
        per_tool_time = {
            tool: {
                "count": len(times),
                "mean_s": float(np.mean(times)),
                "max_s": float(np.max(times)),
            }
            for tool, times in tmp.items()
        }
        per_case_tool_time = {
            case: {
                tool: {
                    "count": len(times),
                    "mean_s": float(np.mean(times)),
                    "max_s": float(np.max(times)),
                }
                for tool, times in tool_map.items()
            }
            for case, tool_map in tmp_case.items()
        }

    return {
        "summary": {
            "total_evaluated": len(test_data),
            "latency": {
                "mean": float(np.mean(latencies)),
                "std": float(np.std(latencies)),
                "min": float(np.min(latencies)),
                "max": float(np.max(latencies)),
                "p50": float(np.percentile(latencies, 50)),
                "p90": float(np.percentile(latencies, 90)),
                "p99": float(np.percentile(latencies, 99))
            },
            "memory": {
                "initial_mb": initial_memory,
                "final_mb": final_memory,
                "peak_mb": float(np.max(memory_usage)),
                "mean_mb": float(np.mean(memory_usage)),
                "growth_mb": final_memory - initial_memory
            },
            "success_rate": success_rate,
            "total_successes": sum(successes),
            "total_failures": len(successes) - sum(successes),
            "weighted_latency_mean": weighted_latency
        },
        "details": results,
        "by_category": calculate_category_performance(results),
        "by_case_latency": by_case_latency,
        "by_tool_time": per_tool_time,
        "by_case_tool_time": per_case_tool_time
    }


def evaluate_system_from_runs(
    runs: List[Dict[str, Any]],
    meta: Dict[str, Any],
    case_weights: Dict[str, float] = None
) -> Dict[str, Any]:
    """ëŸ¬ë„ˆì—ì„œ ìˆ˜ì§‘í•œ runs ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§‘ê³„"""
    latencies = []
    memory_usage = []
    successes = []
    results = []
    by_case_latencies = {}
    conv_case_map = {}

    for run in runs:
        qobj = run.get("item", {})
        ctype = classify_case(qobj, meta)
        conv_id = run.get("conversation_id")
        conv_case_map[conv_id] = ctype

        lat = run.get("latency", 0)
        latencies.append(lat)
        mem_after = run.get("memory_after", 0)
        memory_usage.append(mem_after)
        success = run.get("error") is None
        successes.append(success)
        results.append({
            "question": qobj.get("question"),
            "category": qobj.get("category", "unknown"),
            "case": ctype,
            "latency": lat,
            "memory_before": run.get("memory_before"),
            "memory_after": mem_after,
            "memory_delta": (mem_after - run.get("memory_before", mem_after)) if run.get("memory_before") is not None else 0,
            "success": success,
            "error": run.get("error")
        })
        by_case_latencies.setdefault(ctype, []).append(lat)

    final_memory = memory_usage[-1] if memory_usage else 0
    initial_memory = memory_usage[0] if memory_usage else 0
    success_rate = sum(successes) / len(successes) if successes else 0

    by_case_latency = {
        c: {
            "count": len(vals),
            "mean": float(np.mean(vals)),
            "p90": float(np.percentile(vals, 90)),
        }
        for c, vals in by_case_latencies.items()
    }
    weighted_latency = None
    if case_weights:
        total_w = sum(case_weights.values())
        if total_w > 0:
            weighted_latency = sum(
                case_weights.get(c, 0) / total_w * by_case_latency.get(c, {}).get("mean", 0)
                for c in case_weights.keys()
            )

    # íˆ´ íƒ€ì´ë°: runsì— í¬í•¨ëœ tool_timings ì‚¬ìš©
    per_tool_time = {}
    per_case_tool_time = {}
    from collections import defaultdict
    tmp = defaultdict(list)
    tmp_case = defaultdict(lambda: defaultdict(list))
    for run in runs:
        conv_id = run.get("conversation_id")
        case = conv_case_map.get(conv_id, "unknown")
        for rec in run.get("tool_timings", []):
            tool_name = rec.get("tool", "unknown_tool")
            dur = rec.get("duration", 0)
            tmp[tool_name].append(dur)
            tmp_case[case][tool_name].append(dur)
    if tmp:
        per_tool_time = {
            tool: {
                "count": len(times),
                "mean_s": float(np.mean(times)),
                "max_s": float(np.max(times)),
            }
            for tool, times in tmp.items()
        }
    if tmp_case:
        per_case_tool_time = {
            case: {
                tool: {
                    "count": len(times),
                    "mean_s": float(np.mean(times)),
                    "max_s": float(np.max(times)),
                }
                for tool, times in tool_map.items()
            }
            for case, tool_map in tmp_case.items()
        }

    return {
        "summary": {
            "total_evaluated": len(latencies),
            "latency": {
                "mean": float(np.mean(latencies)) if latencies else 0.0,
                "std": float(np.std(latencies)) if latencies else 0.0,
                "min": float(np.min(latencies)) if latencies else 0.0,
                "max": float(np.max(latencies)) if latencies else 0.0,
                "p50": float(np.percentile(latencies, 50)) if latencies else 0.0,
                "p90": float(np.percentile(latencies, 90)) if latencies else 0.0,
                "p99": float(np.percentile(latencies, 99)) if latencies else 0.0
            },
            "memory": {
                "initial_mb": initial_memory,
                "final_mb": final_memory,
                "peak_mb": float(np.max(memory_usage)) if memory_usage else 0.0,
                "mean_mb": float(np.mean(memory_usage)) if memory_usage else 0.0,
                "growth_mb": final_memory - initial_memory
            },
            "success_rate": success_rate,
            "total_successes": sum(successes),
            "total_failures": len(successes) - sum(successes),
            "weighted_latency_mean": weighted_latency
        },
        "details": results,
        "by_category": calculate_category_performance(results),
        "by_case_latency": by_case_latency,
        "by_tool_time": per_tool_time,
        "by_case_tool_time": per_case_tool_time
    }


def calculate_category_performance(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ í†µê³„"""
    categories = {}

    for result in results:
        category = result.get("category", "unknown")
        if category not in categories:
            categories[category] = {
                "latencies": [],
                "successes": []
            }

        categories[category]["latencies"].append(result["latency"])
        categories[category]["successes"].append(result["success"])

    stats = {}
    for category, data in categories.items():
        stats[category] = {
            "count": len(data["latencies"]),
            "avg_latency": float(np.mean(data["latencies"])),
            "success_rate": sum(data["successes"]) / len(data["successes"]) if data["successes"] else 0
        }

    return stats


def main():
    """ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰"""
    import argparse
    parser = argparse.ArgumentParser(description="ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€")
    parser.add_argument("--sample", "-s", type=int, help="ìƒ˜í”Œ í¬ê¸° (ì „ì²´ ëŒ€ì‹  ì¼ë¶€ë§Œ)")
    parser.add_argument("--case-weights", type=str, help="ì¼€ì´ìŠ¤ë³„ ê°€ì¤‘ì¹˜ JSON ê²½ë¡œ(ì˜ˆ: {'case2':0.3,'web':0.2,...})")
    args = parser.parse_args()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    dataset_path = Path(__file__).parent.parent / "datasets" / "test_questions_prompt_pruned.json"

    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    test_questions = data["questions"]
    meta = data.get("metadata", {})

    case_weights = None
    if args.case_weights:
        try:
            with open(args.case_weights, "r", encoding="utf-8") as f:
                case_weights = json.load(f)
        except Exception as e:
            print(f"âš ï¸ ì¼€ì´ìŠ¤ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # Agent ì´ˆê¸°í™”
    try:
        from agent.agent import create_agent
        agent = create_agent()

        results = evaluate_system_performance(agent, test_questions, sample_size=args.sample, meta=meta, case_weights=case_weights)

        # ê²°ê³¼ ì €ì¥
        output_path = Path(__file__).parent.parent / "results" / "system_evaluation.json"
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ: {output_path}")
        return results

    except ImportError as e:
        print(f"âš ï¸ Agent ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        print("ë°±ì—”ë“œì˜ agents ëª¨ë“ˆ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return {"error": str(e)}


if __name__ == "__main__":
    results = main()
    if "summary" in results:
        print("\n=== ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìš”ì•½ ===")
        summary = results["summary"]
        print(f"í‰ê°€ ì§ˆë¬¸ ìˆ˜: {summary['total_evaluated']}")
        print(f"\nì‘ë‹µ ì‹œê°„:")
        print(f"  í‰ê· : {summary['latency']['mean']:.2f}s (Â±{summary['latency']['std']:.2f}s)")
        print(f"  P50: {summary['latency']['p50']:.2f}s")
        print(f"  P90: {summary['latency']['p90']:.2f}s")
        print(f"  P99: {summary['latency']['p99']:.2f}s")
        print(f"  ìµœì†Œ/ìµœëŒ€: {summary['latency']['min']:.2f}s / {summary['latency']['max']:.2f}s")
        print(f"\në©”ëª¨ë¦¬:")
        print(f"  ì´ˆê¸°: {summary['memory']['initial_mb']:.1f} MB")
        print(f"  ìµœì¢…: {summary['memory']['final_mb']:.1f} MB")
        print(f"  í”¼í¬: {summary['memory']['peak_mb']:.1f} MB")
        print(f"  ì¦ê°€ëŸ‰: {summary['memory']['growth_mb']:.1f} MB")
        print(f"\nì„±ê³µë¥ : {summary['success_rate']:.1%} ({summary['total_successes']}/{summary['total_evaluated']})")

        if "by_category" in results:
            print("\nì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
            for category, stats in results["by_category"].items():
                print(f"  {category}: {stats['avg_latency']:.2f}s, ì„±ê³µë¥  {stats['success_rate']:.1%} (n={stats['count']})")
