"""
ì „ì²´ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  í‰ê°€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸/ë°±ì—”ë“œ ê²½ë¡œ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(ROOT_DIR))  # evaluation.* import ê°€ëŠ¥
sys.path.insert(0, str(ROOT_DIR / "backend"))

from evaluation.scripts.evaluate_answer import evaluate_answer_quality
from evaluation.scripts.evaluate_tools import evaluate_tool_accuracy, ToolCallLogger
from evaluation.scripts.evaluate_system import evaluate_system_performance
from evaluation.scripts.eval_cases import load_dataset, partition_by_case


def run_all_evaluations(
    output_dir: str = None,
    skip_rag: bool = False,
    skip_answer: bool = False,
    skip_tools: bool = False,
    skip_system: bool = False,
    sample_size: int = None,
    system_sample: int = 20
):
    """ëª¨ë“  í‰ê°€ ì‹¤í–‰"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path(__file__).parent.parent / "results"

    output_path.mkdir(parents=True, exist_ok=True)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    data = load_dataset()
    test_questions = data["questions"]
    meta = data.get("metadata", {})

    # ìƒ˜í”Œ í¬ê¸° ì ìš©
    if sample_size and sample_size < len(test_questions):
        import random
        test_questions = random.sample(test_questions, sample_size)
        print(f"ğŸ“Š ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ ì§ˆë¬¸\n")

    # Agent ì´ˆê¸°í™”
    try:
        from agent.agent import create_agent
        agent = create_agent()
    except ImportError as e:
        print(f"âŒ Agent ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

    results = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(test_questions),
            "sample_size": sample_size
        }
    }

    # 1. RAG í‰ê°€ (Case2 ëŒ€ìƒ)
    if not skip_rag:
        try:
            from evaluation.scripts.evaluate_rag import evaluate_rag_quality, _build_retriever
            print("=" * 50)
            print("1. RAG ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€")
            print("=" * 50)
            retriever = _build_retriever()
            if retriever:
                case2_questions = [
                    q for q in test_questions
                    if "search_facilities" in (q.get("expected_tools") or [])
                    and q.get("relevant_doc_ids")
                ]
                print(f"RAG í‰ê°€ ëŒ€ìƒ ë¬¸í•­: {len(case2_questions)}ê°œ")
                rag_results = evaluate_rag_quality(retriever, case2_questions, k=3)
                results["rag"] = rag_results
                with open(output_path / "rag_evaluation.json", "w", encoding="utf-8") as f:
                    json.dump(rag_results, f, ensure_ascii=False, indent=2)
                print(f"âœ… RAG í‰ê°€ ì™„ë£Œ: {len(case2_questions)}ê°œ ë¬¸í•­\n")
            else:
                results["rag"] = {"skipped": True, "reason": "retriever unavailable"}
                print("âš ï¸ RAG retrieverë¥¼ êµ¬ì„±í•˜ì§€ ëª»í•´ ìŠ¤í‚µë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            print()
        except Exception as e:
            results["rag"] = {"error": str(e)}
            print(f"âš ï¸ RAG í‰ê°€ ì‹¤íŒ¨: {e}\n")

    # 2. ë‹µë³€ í’ˆì§ˆ í‰ê°€
    if not skip_answer:
        print("=" * 50)
        print("2. ë‹µë³€ í’ˆì§ˆ í‰ê°€ (LLM-as-Judge)")
        print("=" * 50)
        answer_results = evaluate_answer_quality(agent, test_questions)
        results["answer_quality"] = answer_results

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "answer_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(answer_results, f, ensure_ascii=False, indent=2)
        print()

    # 3. Tool ì •í™•ë„ í‰ê°€
    if not skip_tools:
        print("=" * 50)
        print("3. Tool ì‚¬ìš© ì •í™•ë„ í‰ê°€")
        print("=" * 50)
        tool_logger = ToolCallLogger()
        tool_results = evaluate_tool_accuracy(agent, test_questions, tool_logger, meta=meta)
        results["tool_accuracy"] = tool_results
        if "service_quality" in tool_results:
            results["service_quality"] = tool_results["service_quality"]

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "tool_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(tool_results, f, ensure_ascii=False, indent=2)
        print()

    # 4. ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€
    if not skip_system:
        print("=" * 50)
        print("4. ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€")
        print("=" * 50)
        sys_sample = system_sample if system_sample and system_sample > 0 else None
        system_results = evaluate_system_performance(agent, test_questions, sample_size=sys_sample)
        results["system_performance"] = system_results

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "system_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(system_results, f, ensure_ascii=False, indent=2)
        print()

    # ì¢…í•© ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    combined_output = output_path / f"evaluation_report_{timestamp}.json"
    with open(combined_output, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    generate_markdown_report(results, output_path / f"evaluation_report_{timestamp}.md")

    print("=" * 50)
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   JSON ë¦¬í¬íŠ¸: {combined_output}")
    print(f"   MD ë¦¬í¬íŠ¸: {output_path / f'evaluation_report_{timestamp}.md'}")
    print("=" * 50)

    return results


def generate_markdown_report(results: dict, output_path: Path):
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ ìƒì„±"""

    md = []
    md.append("# ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸\n")
    md.append(f"**í‰ê°€ ì¼ì‹œ**: {results['metadata']['timestamp']}\n")
    md.append(f"**í‰ê°€ ì§ˆë¬¸ ìˆ˜**: {results['metadata']['total_questions']}\n")
    md.append("---\n")

    # ë‹µë³€ í’ˆì§ˆ
    if "answer_quality" in results and "summary" in results["answer_quality"]:
        summary = results["answer_quality"]["summary"]
        md.append("## 1. ë‹µë³€ í’ˆì§ˆ (LLM-as-Judge)\n")
        md.append("| í•­ëª© | í‰ê·  | í‘œì¤€í¸ì°¨ | ìµœì†Œ | ìµœëŒ€ |")
        md.append("|------|------|----------|------|------|")
        md.append(f"| ì •í™•ì„± | {summary['accuracy']['mean']:.2f} | {summary['accuracy']['std']:.2f} | {summary['accuracy']['min']:.0f} | {summary['accuracy']['max']:.0f} |")
        md.append(f"| ê´€ë ¨ì„± | {summary['relevance']['mean']:.2f} | {summary['relevance']['std']:.2f} | {summary['relevance']['min']:.0f} | {summary['relevance']['max']:.0f} |")
        md.append(f"| ìœ ìš©ì„± | {summary['usefulness']['mean']:.2f} | {summary['usefulness']['std']:.2f} | {summary['usefulness']['min']:.0f} | {summary['usefulness']['max']:.0f} |")
        md.append(f"| **ì¢…í•©** | **{summary['overall']['mean']:.2f}** | {summary['overall']['std']:.2f} | {summary['overall']['min']:.0f} | {summary['overall']['max']:.0f} |")
        md.append("\n")

    # Tool ì •í™•ë„
    if "tool_accuracy" in results and "summary" in results["tool_accuracy"]:
        summary = results["tool_accuracy"]["summary"]
        md.append("## 2. Tool ì‚¬ìš© ì •í™•ë„\n")
        md.append(f"- **Tool ì„ íƒ ì •í™•ë„**: {summary['tool_selection_accuracy']['mean']:.1%}")
        md.append(f"- **íŒŒë¼ë¯¸í„° ì •í™•ë„**: {summary['parameter_accuracy']['mean']:.1%}")
        md.append(f"- **ì¢…í•© ì •í™•ë„**: {summary['combined_accuracy']['mean']:.1%}")
        md.append("\n")

        if "by_category" in results["tool_accuracy"]:
            md.append("### ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„\n")
            md.append("| ì¹´í…Œê³ ë¦¬ | ì§ˆë¬¸ ìˆ˜ | Tool ì„ íƒ | íŒŒë¼ë¯¸í„° |")
            md.append("|----------|---------|-----------|----------|")
            for cat, stats in results["tool_accuracy"]["by_category"].items():
                md.append(f"| {cat} | {stats['count']} | {stats['selection_accuracy']:.1%} | {stats['parameter_accuracy']:.1%} |")
            md.append("\n")
        if "by_tool" in results["tool_accuracy"]:
            md.append("### íˆ´ë³„ í˜¸ì¶œ ì„±ê³µë¥ \n")
            md.append("| íˆ´ | ê¸°ëŒ€ í˜¸ì¶œ | ì‹¤ì œ í˜¸ì¶œ(íˆíŠ¸) | ì„±ê³µë¥  |")
            md.append("|----|----------|---------------|--------|")
            for tool, stats in results["tool_accuracy"]["by_tool"].items():
                md.append(f"| {tool} | {stats.get('expected',0)} | {stats.get('hit',0)} | {stats.get('success_rate',0):.1%} |")
            md.append("\n")

    # ì„œë¹„ìŠ¤ í’ˆì§ˆ ìš”ì•½ (ì¼€ì´ìŠ¤ë³„ ì„±ê³µë¥ )
    if "service_quality" in results and "by_case" in results["service_quality"]:
        md.append("## 3. ì„œë¹„ìŠ¤ í’ˆì§ˆ (ì¼€ì´ìŠ¤ë³„)\n")
        md.append("| ì¼€ì´ìŠ¤ | ì„±ê³µë¥  | ìƒ˜í”Œ ìˆ˜ | ì§€í‘œ ì„¤ëª… |")
        md.append("|--------|--------|---------|-----------|")
        for case, stats in results["service_quality"]["by_case"].items():
            desc = stats.get("description", "")
            md.append(f"| {case} | {stats.get('success_rate',0):.1%} | {stats.get('count',0)} | {desc} |")
        md.append("\n")

    # RAG ê²€ìƒ‰ í’ˆì§ˆ
    if "rag" in results and "summary" in results["rag"]:
        summary = results["rag"]["summary"]
        md.append("## 4. RAG ê²€ìƒ‰ í’ˆì§ˆ\n")
        md.append(f"- Precision@{summary['k']}: {summary['precision_at_k']['mean']:.3f}")
        md.append(f"- MRR: {summary['mrr']['mean']:.3f}")
        md.append("\n")

    # ì‹œìŠ¤í…œ ì„±ëŠ¥
    if "system_performance" in results and "summary" in results["system_performance"]:
        summary = results["system_performance"]["summary"]
        md.append("## 5. ì‹œìŠ¤í…œ ì„±ëŠ¥\n")
        md.append("### ì‘ë‹µ ì‹œê°„\n")
        md.append(f"- í‰ê· : **{summary['latency']['mean']:.2f}s** (Â±{summary['latency']['std']:.2f}s)")
        md.append(f"- P50: {summary['latency']['p50']:.2f}s")
        md.append(f"- P90: {summary['latency']['p90']:.2f}s")
        md.append(f"- P99: {summary['latency']['p99']:.2f}s")
        md.append("\n### ë©”ëª¨ë¦¬\n")
        md.append(f"- ì´ˆê¸°: {summary['memory']['initial_mb']:.1f} MB")
        md.append(f"- ìµœì¢…: {summary['memory']['final_mb']:.1f} MB")
        md.append(f"- í”¼í¬: {summary['memory']['peak_mb']:.1f} MB")
        md.append(f"\n### ì„±ê³µë¥ : **{summary['success_rate']:.1%}** ({summary['total_successes']}/{summary['total_evaluated']})\n")

    # íŒŒì¼ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md))


def main():
    parser = argparse.ArgumentParser(description="Kids Chatbot ì„±ëŠ¥ í‰ê°€")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--sample", "-s", type=int, help="ìƒ˜í”Œ í¬ê¸° (ì „ì²´ í‰ê°€ ëŒ€ì‹  ì¼ë¶€ë§Œ)")
    parser.add_argument("--skip-rag", action="store_true", help="RAG í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-answer", action="store_true", help="ë‹µë³€ í’ˆì§ˆ í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-tools", action="store_true", help="Tool ì •í™•ë„ í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-system", action="store_true", help="ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--system-sample", type=int, default=20, help="ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìƒ˜í”Œ í¬ê¸° (ê¸°ë³¸ 20, 0/ìŒìˆ˜ë©´ ì „ì²´)")

    args = parser.parse_args()

    run_all_evaluations(
        output_dir=args.output,
        skip_rag=args.skip_rag,
        skip_answer=args.skip_answer,
        skip_tools=args.skip_tools,
        skip_system=args.skip_system,
        sample_size=args.sample,
        system_sample=args.system_sample
    )


if __name__ == "__main__":
    main()
