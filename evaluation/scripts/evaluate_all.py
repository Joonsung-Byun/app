"""
ì „ì²´ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  í‰ê°€ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
"""

import json
import sys
import os
from pathlib import Path
from datetime import datetime
import argparse

# ë°±ì—”ë“œ ëª¨ë“ˆ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "backend"))

from evaluate_answer import evaluate_answer_quality
from evaluate_tools import evaluate_tool_accuracy, ToolCallLogger
from evaluate_system import evaluate_system_performance


def run_all_evaluations(
    output_dir: str = None,
    skip_rag: bool = False,
    skip_answer: bool = False,
    skip_tools: bool = False,
    skip_system: bool = False,
    sample_size: int = None
):
    """ëª¨ë“  í‰ê°€ ì‹¤í–‰"""

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path(__file__).parent.parent / "results"

    output_path.mkdir(parents=True, exist_ok=True)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    dataset_path = Path(__file__).parent.parent / "datasets" / "test_questions.json"
    with open(dataset_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    test_questions = data["questions"]

    # ìƒ˜í”Œ í¬ê¸° ì ìš©
    if sample_size and sample_size < len(test_questions):
        import random
        test_questions = random.sample(test_questions, sample_size)
        print(f"ğŸ“Š ìƒ˜í”Œ í¬ê¸°: {sample_size}ê°œ ì§ˆë¬¸\n")

    # Agent ì´ˆê¸°í™”
    try:
        from agent.agent import create_agent
        agent = create_agent()
    except ImportError as e:
        print(f"âŒ Agent ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return {"error": str(e)}

    results = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(test_questions),
            "sample_size": sample_size
        }
    }

    # 1. RAG í‰ê°€ (í˜„ì¬ ìŠ¤í‚µ - relevant_doc_ids í•„ìš”)
    if not skip_rag:
        print("=" * 50)
        print("1. RAG ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€")
        print("=" * 50)
        print("âš ï¸ RAG í‰ê°€ëŠ” relevant_doc_idsê°€ í•„ìš”í•©ë‹ˆë‹¤. ìŠ¤í‚µë©ë‹ˆë‹¤.\n")
        results["rag"] = {"skipped": True, "reason": "relevant_doc_ids not set"}

    # 2. ë‹µë³€ í’ˆì§ˆ í‰ê°€
    if not skip_answer:
        print("=" * 50)
        print("2. ë‹µë³€ í’ˆì§ˆ í‰ê°€ (LLM-as-Judge)")
        print("=" * 50)
        answer_results = evaluate_answer_quality(agent, test_questions)
        results["answer_quality"] = answer_results

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "answer_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(answer_results, f, ensure_ascii=False, indent=2)
        print()

    # 3. Tool ì •í™•ë„ í‰ê°€
    if not skip_tools:
        print("=" * 50)
        print("3. Tool ì‚¬ìš© ì •í™•ë„ í‰ê°€")
        print("=" * 50)
        tool_logger = ToolCallLogger()
        tool_results = evaluate_tool_accuracy(agent, test_questions, tool_logger)
        results["tool_accuracy"] = tool_results

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "tool_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(tool_results, f, ensure_ascii=False, indent=2)
        print()

    # 4. ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€
    if not skip_system:
        print("=" * 50)
        print("4. ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€")
        print("=" * 50)
        system_results = evaluate_system_performance(agent, test_questions)
        results["system_performance"] = system_results

        # ê°œë³„ ê²°ê³¼ ì €ì¥
        with open(output_path / "system_evaluation.json", "w", encoding="utf-8") as f:
            json.dump(system_results, f, ensure_ascii=False, indent=2)
        print()

    # ì¢…í•© ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    combined_output = output_path / f"evaluation_report_{timestamp}.json"
    with open(combined_output, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    generate_markdown_report(results, output_path / f"evaluation_report_{timestamp}.md")

    print("=" * 50)
    print(f"âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   JSON ë¦¬í¬íŠ¸: {combined_output}")
    print(f"   MD ë¦¬í¬íŠ¸: {output_path / f'evaluation_report_{timestamp}.md'}")
    print("=" * 50)

    return results


def generate_markdown_report(results: dict, output_path: Path):
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¦¬í¬íŠ¸ ìƒì„±"""

    md = []
    md.append("# ì„±ëŠ¥ í‰ê°€ ë¦¬í¬íŠ¸\n")
    md.append(f"**í‰ê°€ ì¼ì‹œ**: {results['metadata']['timestamp']}\n")
    md.append(f"**í‰ê°€ ì§ˆë¬¸ ìˆ˜**: {results['metadata']['total_questions']}\n")
    md.append("---\n")

    # ë‹µë³€ í’ˆì§ˆ
    if "answer_quality" in results and "summary" in results["answer_quality"]:
        summary = results["answer_quality"]["summary"]
        md.append("## 1. ë‹µë³€ í’ˆì§ˆ (LLM-as-Judge)\n")
        md.append("| í•­ëª© | í‰ê·  | í‘œì¤€í¸ì°¨ | ìµœì†Œ | ìµœëŒ€ |")
        md.append("|------|------|----------|------|------|")
        md.append(f"| ì •í™•ì„± | {summary['accuracy']['mean']:.2f} | {summary['accuracy']['std']:.2f} | {summary['accuracy']['min']:.0f} | {summary['accuracy']['max']:.0f} |")
        md.append(f"| ê´€ë ¨ì„± | {summary['relevance']['mean']:.2f} | {summary['relevance']['std']:.2f} | {summary['relevance']['min']:.0f} | {summary['relevance']['max']:.0f} |")
        md.append(f"| ìœ ìš©ì„± | {summary['usefulness']['mean']:.2f} | {summary['usefulness']['std']:.2f} | {summary['usefulness']['min']:.0f} | {summary['usefulness']['max']:.0f} |")
        md.append(f"| **ì¢…í•©** | **{summary['overall']['mean']:.2f}** | {summary['overall']['std']:.2f} | {summary['overall']['min']:.0f} | {summary['overall']['max']:.0f} |")
        md.append("\n")

    # Tool ì •í™•ë„
    if "tool_accuracy" in results and "summary" in results["tool_accuracy"]:
        summary = results["tool_accuracy"]["summary"]
        md.append("## 2. Tool ì‚¬ìš© ì •í™•ë„\n")
        md.append(f"- **Tool ì„ íƒ ì •í™•ë„**: {summary['tool_selection_accuracy']['mean']:.1%}")
        md.append(f"- **íŒŒë¼ë¯¸í„° ì •í™•ë„**: {summary['parameter_accuracy']['mean']:.1%}")
        md.append(f"- **ì¢…í•© ì •í™•ë„**: {summary['combined_accuracy']['mean']:.1%}")
        md.append("\n")

        if "by_category" in results["tool_accuracy"]:
            md.append("### ì¹´í…Œê³ ë¦¬ë³„ ì •í™•ë„\n")
            md.append("| ì¹´í…Œê³ ë¦¬ | ì§ˆë¬¸ ìˆ˜ | Tool ì„ íƒ | íŒŒë¼ë¯¸í„° |")
            md.append("|----------|---------|-----------|----------|")
            for cat, stats in results["tool_accuracy"]["by_category"].items():
                md.append(f"| {cat} | {stats['count']} | {stats['selection_accuracy']:.1%} | {stats['parameter_accuracy']:.1%} |")
            md.append("\n")

    # ì‹œìŠ¤í…œ ì„±ëŠ¥
    if "system_performance" in results and "summary" in results["system_performance"]:
        summary = results["system_performance"]["summary"]
        md.append("## 3. ì‹œìŠ¤í…œ ì„±ëŠ¥\n")
        md.append("### ì‘ë‹µ ì‹œê°„\n")
        md.append(f"- í‰ê· : **{summary['latency']['mean']:.2f}s** (Â±{summary['latency']['std']:.2f}s)")
        md.append(f"- P50: {summary['latency']['p50']:.2f}s")
        md.append(f"- P90: {summary['latency']['p90']:.2f}s")
        md.append(f"- P99: {summary['latency']['p99']:.2f}s")
        md.append("\n### ë©”ëª¨ë¦¬\n")
        md.append(f"- ì´ˆê¸°: {summary['memory']['initial_mb']:.1f} MB")
        md.append(f"- ìµœì¢…: {summary['memory']['final_mb']:.1f} MB")
        md.append(f"- í”¼í¬: {summary['memory']['peak_mb']:.1f} MB")
        md.append(f"\n### ì„±ê³µë¥ : **{summary['success_rate']:.1%}** ({summary['total_successes']}/{summary['total_evaluated']})\n")

    # íŒŒì¼ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md))


def main():
    parser = argparse.ArgumentParser(description="Kids Chatbot ì„±ëŠ¥ í‰ê°€")
    parser.add_argument("--output", "-o", type=str, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--sample", "-s", type=int, help="ìƒ˜í”Œ í¬ê¸° (ì „ì²´ í‰ê°€ ëŒ€ì‹  ì¼ë¶€ë§Œ)")
    parser.add_argument("--skip-rag", action="store_true", help="RAG í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-answer", action="store_true", help="ë‹µë³€ í’ˆì§ˆ í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-tools", action="store_true", help="Tool ì •í™•ë„ í‰ê°€ ìŠ¤í‚µ")
    parser.add_argument("--skip-system", action="store_true", help="ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ ìŠ¤í‚µ")

    args = parser.parse_args()

    run_all_evaluations(
        output_dir=args.output,
        skip_rag=args.skip_rag,
        skip_answer=args.skip_answer,
        skip_tools=args.skip_tools,
        skip_system=args.skip_system,
        sample_size=args.sample
    )


if __name__ == "__main__":
    main()
