from typing import List
import joblib
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig
from config import settings

class PCAEmbeddings:
    def __init__(self):
        print("="*70)
        print("ðŸš€ PCA ìž„ë² ë”© ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print("="*70)
        
        # PCA ëª¨ë¸ ë¡œë“œ
        print(f"ðŸ“¥ PCA ëª¨ë¸ ë¡œë”©: {settings.PCA_MODEL_PATH}")
        self.pca = joblib.load(settings.PCA_MODEL_PATH)
        print(f"âœ… PCA ë¡œë“œ ì™„ë£Œ! (ìž…ë ¥: {self.pca.n_features_in_}, ì¶œë ¥: {self.pca.n_components_})")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        self.device = "cuda" if (settings.USE_GPU and torch.cuda.is_available()) else "cpu"
        print(f"ðŸ“± Device: {self.device}")
        
        if self.device == "cpu":
            print("âš ï¸  ê²½ê³ : CPU ëª¨ë“œìž…ë‹ˆë‹¤. GPU ì‚¬ìš©ì„ ê¶Œìž¥í•©ë‹ˆë‹¤!")
        
        # Alibaba GTE ëª¨ë¸ ë¡œë“œ
        print(f"ðŸ“¥ ìž„ë² ë”© ëª¨ë¸ ë¡œë”©: {settings.EMBEDDING_MODEL}")
        print(f"âš ï¸  7B ëª¨ë¸ ë¡œë”© ì¤‘... ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤")
        
        self.tokenizer = AutoTokenizer.from_pretrained(settings.EMBEDDING_MODEL)
        
        # 4-bit quantization ì„¤ì • (GPU 8GBìš©)
        if self.device == "cuda":
            print("ðŸ”§ 4-bit quantization ì ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            
            self.model = AutoModel.from_pretrained(
                settings.EMBEDDING_MODEL,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            # CPU ëª¨ë“œ
            self.model = AutoModel.from_pretrained(
                settings.EMBEDDING_MODEL,
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
            self.model = self.model.to(self.device)
        
        self.model.eval()
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        print("="*70)
    
    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def _get_gte_embedding(self, text: str) -> np.ndarray:
        """Alibaba GTE ìž„ë² ë”© ìƒì„±"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
        
        embedding = embeddings.cpu().numpy()[0]
        embedding = embedding / np.linalg.norm(embedding)
        
        return embedding
    
    def _adjust_dimension(self, embedding: np.ndarray) -> np.ndarray:
        """ì°¨ì› ì¡°ì •"""
        target_dim = self.pca.n_features_in_
        current_dim = len(embedding)
        
        if current_dim == target_dim:
            return embedding
        elif current_dim > target_dim:
            return embedding[:target_dim]
        else:
            return np.pad(embedding, (0, target_dim - current_dim), mode='constant')
    
    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ìž„ë² ë”©"""
        embedding = self._get_gte_embedding(text)
        embedding = self._adjust_dimension(embedding)
        pca_embedding = self.pca.transform([embedding])[0]
        return pca_embedding.tolist()
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ì—¬ëŸ¬ ë¬¸ì„œ ìž„ë² ë”©"""
        embeddings = []
        for i, text in enumerate(texts):
            if (i + 1) % 100 == 0:
                print(f"   ìž„ë² ë”© ì§„í–‰: {i+1}/{len(texts)}")
            embedding = self._get_gte_embedding(text)
            embeddings.append(embedding)
        
        embeddings = np.array(embeddings)
        adjusted_embeddings = np.array([self._adjust_dimension(emb) for emb in embeddings])
        pca_embeddings = self.pca.transform(adjusted_embeddings)
        return pca_embeddings.tolist()

pca_embeddings = PCAEmbeddings()