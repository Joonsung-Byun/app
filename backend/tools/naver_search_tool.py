import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List
from config import settings
from datetime import datetime, timedelta
from models.chat_models import get_llm
from utils.conversation_memory import save_search_results, get_shown_facility_names, set_status

FULL_MOBILE_USER_AGENT = "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"

# ============================================
# 1. ë¹„ë™ê¸° í¬ë¡¤ë§ í—¬í¼ í•¨ìˆ˜
# ============================================
async def fetch_single_blog(session, link: str) -> str:
    """ê°œë³„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ë¥¼ ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§."""
    try:
        target_link = link
        if "blog.naver.com" in link and "m.blog.naver.com" not in link:
            target_link = link.replace("blog.naver.com", "m.blog.naver.com")

        headers = {
            "User-Agent": FULL_MOBILE_USER_AGENT
        }
        async with session.get(target_link, headers=headers, timeout=3) as resp:
            if resp.status != 200: return ""
            html = await resp.text()
            soup = BeautifulSoup(html, "html.parser")
            
            content = soup.find("div", class_="se-main-container")
            if not content:
                content = soup.find("div", id="postViewArea")

            if content:
                return content.get_text(" ", strip=True)[:850] 

            return ""

    except Exception as e:
        return ""

async def fetch_multiple_urls(links: List[str]) -> List[str]:
    """ì—¬ëŸ¬ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ë™ì‹œì—(ë³‘ë ¬ë¡œ) í¬ë¡¤ë§."""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_single_blog(session, link) for link in links]
        return await asyncio.gather(*tasks)

# ============================================
# 2. ë‚ ì§œ ê³„ì‚° ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================
WEEKDAY_MAP = {
    "ì›”ìš”ì¼": 0, "ì›”": 0, "í™”ìš”ì¼": 1, "í™”": 1, "ìˆ˜ìš”ì¼": 2, "ìˆ˜": 2,
    "ëª©ìš”ì¼": 3, "ëª©": 3, "ê¸ˆìš”ì¼": 4, "ê¸ˆ": 4, "í† ìš”ì¼": 5, "í† ": 5, "ì¼ìš”ì¼": 6, "ì¼": 6
}

def calculate_date_range(keyword: str) -> str:
    today = datetime.now().date()
    if "ì˜¤ëŠ˜" in keyword: return f"ê¸°ê°„: {today.strftime('%Y.%m.%d')}"
    elif "ë‚´ì¼" in keyword: return f"ê¸°ê°„: {(today + timedelta(days=1)).strftime('%Y.%m.%d')}"
    elif "ì´ë²ˆ ì£¼ë§" in keyword or "ì£¼ë§" in keyword:
        current_weekday = today.weekday() 
        days_until_saturday = 5 - current_weekday
        if days_until_saturday < 0: days_until_saturday += 7 
        saturday = today + timedelta(days=days_until_saturday)
        sunday = saturday + timedelta(days=1)
        return f"ê¸°ê°„: {saturday.strftime('%Y.%m.%d')} ~ {sunday.strftime('%Y.%m.%d')}"
    elif "ë‹¤ìŒ ì£¼" in keyword and "ì£¼ë§" not in keyword:
        days_until_next_monday = 7 - today.weekday()
        monday_next_week = today + timedelta(days=days_until_next_monday)
        sunday_next_week = monday_next_week + timedelta(days=6)
        return f"ê¸°ê°„: {monday_next_week.strftime('%Y.%m.%d')} ~ {sunday_next_week.strftime('%Y.%m.%d')}"
    elif "ë‹¤ìŒ ì£¼ë§" in keyword:
        days_until_next_monday = 7 - today.weekday()
        monday_next_week = today + timedelta(days=days_until_next_monday)
        return f"ê¸°ê°„: {(monday_next_week + timedelta(days=5)).strftime('%Y.%m.%d')} ~ {(monday_next_week + timedelta(days=6)).strftime('%Y.%m.%d')}"
    if "ì´ë²ˆ ì£¼" in keyword:
        for day_name, day_index in WEEKDAY_MAP.items():
            if f"ì´ë²ˆ ì£¼ {day_name}" in keyword or f"ì´ë²ˆ ì£¼ {day_name[:-2]}" in keyword:
                days_diff = day_index - today.weekday()
                if days_diff < 0: days_diff += 7
                return f"ê¸°ê°„: {(today + timedelta(days=days_diff)).strftime('%Y.%m.%d')}"
    return ""

# ============================================
# 3. AI ë¶„ì„ ë°ì´í„° ëª¨ë¸ 
# ============================================
class SearchResultItem(BaseModel):
    title: str = Field(description="ë¸”ë¡œê·¸ ì œëª©")
    link: str = Field(description="ë¸”ë¡œê·¸ ë§í¬")
    description: str = Field(description="ìš”ì•½ ë‚´ìš©")
    venue: str = Field(description="ì •í™•í•œ ì¥ì†Œëª…(ì—†ìœ¼ë©´ ë¹ˆì¹¸)")

class SearchAnalysisResult(BaseModel):
    results: List[SearchResultItem]

# ============================================
# 4. íˆ´ ì •ì˜ 
# ============================================
@tool
async def naver_web_search(query: str, conversation_id: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ -> ìƒìœ„ ê²°ê³¼ ë³‘ë ¬ í¬ë¡¤ë§ -> ì •í™•í•œ ì¥ì†Œëª… ì¶”ì¶œì˜ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. (ì™„ì „ ë¹„ë™ê¸°)
    """
    naver_client_id = settings.NAVER_CLIENT_ID
    naver_client_secret = settings.NAVER_CLIENT_SECRET
    
    if not naver_client_id or not naver_client_secret:
        return "ì˜¤ë¥˜: configì— ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # [Step 1] ë‚ ì§œ ê³„ì‚° & ì¿¼ë¦¬ ìƒì„±
    date_info = calculate_date_range(query)
    final_query = f"{query} {date_info}" if date_info else query
    
    today = datetime.now()
    if str(today.year) not in final_query and not date_info:
         final_query = f"{today.year}ë…„ {today.month}ì›” {final_query}"

    shown_items = set(get_shown_facility_names(conversation_id)) if conversation_id else set()

    # [Step 2] ë„¤ì´ë²„ API í˜¸ì¶œ 
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {"X-Naver-Client-Id": naver_client_id, "X-Naver-Client-Secret": naver_client_secret}
    params = {"query": final_query, "display": 10, "sort": "sim"}

    try:
        if conversation_id:
            set_status(conversation_id, "ì›¹ ì •ë³´ í™•ì¸ ì¤‘..")

        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, params=params) as resp:
                if resp.status != 200:
                    return f"ë„¤ì´ë²„ API ì˜¤ë¥˜ ë°œìƒ (ìƒíƒœì½”ë“œ: {resp.status})"
                data = await resp.json() 
        
        if not data.get('items'): return f"'{final_query}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        raw_items = []
        for item in data['items']:
            title = item['title'].replace("<b>", "").replace("</b>", "")
            link = item['link']
            if title in shown_items: continue
            
            raw_items.append({
                "title": title,
                "link": link,
                "description": item['description'].replace("<b>", "").replace("</b>", ""),
                "postdate": item.get('postdate', '')
            })

        if not raw_items: return "ìƒˆë¡œìš´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # [Step 3] 1ì°¨ í•„í„°ë§
        llm = get_llm()
        parser = JsonOutputParser(pydantic_object=SearchAnalysisResult)
        
        prompt_filter = PromptTemplate(
            template="""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}" (ë‚ ì§œíŒíŠ¸: {date_info})
ì˜¤ëŠ˜ ë‚ ì§œ: {today_date}

ì•„ë˜ ë¸”ë¡œê·¸ ëª©ë¡ ì¤‘ ê°€ì¥ ê´€ë ¨ì„± ë†’ê³  ìµœì‹  ì •ë³´ì¸ **ìƒìœ„ 3ê°œ**ë§Œ ì„ íƒí•˜ì„¸ìš”.
(ì‘ë…„ ê¸€, ê´‘ê³ , ê´€ë ¨ ì—†ëŠ” ì§€ì—­ ì œì™¸)
ì¶”ë¡  ë° thinkingì€ í•˜ì§€ ë§ê³  ì¶œë ¥ í˜•ì‹ì— ë”°ë¼ ë‹µë³€ë§Œ ì‘ì„±í•˜ì„¸ìš”.

ëª©ë¡:
{raw_data}

ì¶œë ¥ í˜•ì‹: JSON
{format_instructions}
""",
            input_variables=["user_query", "date_info", "today_date", "raw_data"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        raw_text_list = [f"- ì œëª©: {i['title']}\n  ë§í¬: {i['link']}\n  ìš”ì•½: {i['description']}\n  ë‚ ì§œ: {i['postdate']}" for i in raw_items[:10]]
        
        chain = prompt_filter | llm | parser
        analysis = await chain.ainvoke({
            "user_query": query,
            "date_info": date_info,
            "today_date": today.strftime("%Y-%m-%d"),
            "raw_data": "\n\n".join(raw_text_list)
        })
        
        top_3_results = analysis['results']

       # [Step 4] 2ì°¨ ì •ë°€ ë¶„ì„ 
        target_links = [item['link'] for item in top_3_results]
        
        # 3. ë¹„ë™ê¸° í¬ë¡¤ë§ ì‹¤í–‰ 
        crawled_contents = await fetch_multiple_urls(target_links)

        final_output_results = []
        
        for idx, item in enumerate(top_3_results):
            full_text = crawled_contents[idx]
            
            if full_text:
                # 4. LLMì—ê²Œ ë¶„ì„ ìš”ì²­ 
                refine_prompt = f"""
                ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ì½ê³  ë‹¤ìŒ ë‘ ê°€ì§€ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´.
                
                [ë³¸ë¬¸]: {full_text[:850]}...
                
                [ë¯¸ì…˜]
                1. venue: í–‰ì‚¬ê°€ ì—´ë¦¬ëŠ” **ê²€ìƒ‰ ê°€ëŠ¥í•œ ê±´ë¬¼ëª…/ì‹œì„¤ëª…**ë§Œ ì¶”ì¶œí•´. 
                   - [ì¤‘ìš”] 'OOOì„¼í„° 3ì¸µ', 'OOí™€' ì²˜ëŸ¼ ì¸µìˆ˜ë‚˜ í˜¸ìˆ˜ëŠ” ì œë°œ ë¹¼ì¤˜. (ì§€ë„ ê²€ìƒ‰ì— ë°©í•´ë¨)
                   - ì˜ˆ: "ë²¡ìŠ¤ì½” ì œ1ì „ì‹œì¥ 2í™€" (X) -> "ë²¡ìŠ¤ì½” ì œ1ì „ì‹œì¥" (O)
                   - ì¥ì†Œê°€ ì—†ìœ¼ë©´ 'ì¥ì†Œ ë¶ˆëª…'ì´ë¼ê³  ì ì–´.

                2. summary: í–‰ì‚¬ì˜ í•µì‹¬ ì •ë³´(ì¼ì •, ì‹œê°„, ì…ì¥ë£Œ, ê¿€íŒ ë“±)ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´.
                
                [ì¶œë ¥ ì˜ˆì‹œ]
                {{
                    "venue": "ì„±ìˆ˜ë™ ì—ìŠ¤íŒ©í† ë¦¬ Dë™",
                    "summary": "11ì›” 25ì¼ê¹Œì§€ ì§„í–‰ë˜ë©° ì…ì¥ë£ŒëŠ” ë¬´ë£Œì…ë‹ˆë‹¤. ëŒ€ê¸° ì‹œê°„ì´ ê¸°ë‹ˆ ì˜¤í”ˆëŸ°ì„ ì¶”ì²œí•©ë‹ˆë‹¤."
                }}
                """
                
                try:
                    refined_result_msg = await llm.ainvoke(refine_prompt)
                    refined_result = refined_result_msg.content.strip()
                    
                    refined_result = refined_result.replace("```json", "").replace("```", "")
                    refined_data = json.loads(refined_result)
                    
                    if refined_data.get("venue") and "ë¶ˆëª…" not in refined_data["venue"]:
                        item['venue'] = refined_data["venue"]
                    
                    if refined_data.get("summary"):
                        item['description'] = "âœ¨AIìš”ì•½: " + refined_data["summary"]
                        
                except Exception as e:
                    pass
            
            final_output_results.append(item)

        # [Step 5] ê²°ê³¼ ë°˜í™˜
        if conversation_id:
            save_data = [
                {
                    "name": item.get("venue") or item.get("title"),
                    "link": item.get("link"),
                    "desc": item.get("description", "")
                }
                for item in final_output_results
            ]
            save_search_results(conversation_id, save_data)

        result_text = f"ğŸ” '{final_query}' ê²€ìƒ‰ ë° ì •ë°€ ë¶„ì„ ê²°ê³¼:\n\n"
        
        for idx, item in enumerate(final_output_results, 1):
            html_link = f'<a href="{item["link"]}" target="_blank">ğŸ‘‰ ë¸”ë¡œê·¸ ë³´ê¸°</a>'
            result_text += f"{idx}. **{item['title']}**\n"
            result_text += f"   - ğŸ“ ì¥ì†Œ: {item['venue']}\n" 
            result_text += f"   - ğŸ“ ë‚´ìš©: {item['description']}\n"
            result_text += f"   - {html_link}\n\n"
            
        return result_text

    except Exception as e:
        return f"ê²€ìƒ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
