import requests
import json
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List
from config import settings
from datetime import datetime, timedelta
from models.chat_models import get_llm
from utils.conversation_memory import save_search_results, get_shown_facility_names, set_status

from bs4 import BeautifulSoup
import re

# ============================================
# 1. ë‚ ì§œ ê³„ì‚° ìœ í‹¸ë¦¬í‹°
# ============================================
WEEKDAY_MAP = {
    "ì›”ìš”ì¼": 0, "ì›”": 0, "í™”ìš”ì¼": 1, "í™”": 1, "ìˆ˜ìš”ì¼": 2, "ìˆ˜": 2,
    "ëª©ìš”ì¼": 3, "ëª©": 3, "ê¸ˆìš”ì¼": 4, "ê¸ˆ": 4, "í† ìš”ì¼": 5, "í† ": 5, "ì¼ìš”ì¼": 6, "ì¼": 6
}

def calculate_date_range(keyword: str) -> str:
    today = datetime.now().date()
    if "ì˜¤ëŠ˜" in keyword: return f"ê¸°ê°„: {today.strftime('%Y.%m.%d')}"
    elif "ë‚´ì¼" in keyword: return f"ê¸°ê°„: {(today + timedelta(days=1)).strftime('%Y.%m.%d')}"
    elif "ì´ë²ˆ ì£¼ë§" in keyword or "ì£¼ë§" in keyword:
        current_weekday = today.weekday() 
        days_until_saturday = 5 - current_weekday
        if days_until_saturday < 0: days_until_saturday += 7 
        saturday = today + timedelta(days=days_until_saturday)
        sunday = saturday + timedelta(days=1)
        return f"ê¸°ê°„: {saturday.strftime('%Y.%m.%d')} ~ {sunday.strftime('%Y.%m.%d')}"
    elif "ë‹¤ìŒ ì£¼" in keyword and "ì£¼ë§" not in keyword:
        days_until_next_monday = 7 - today.weekday()
        monday_next_week = today + timedelta(days=days_until_next_monday)
        sunday_next_week = monday_next_week + timedelta(days=6)
        return f"ê¸°ê°„: {monday_next_week.strftime('%Y.%m.%d')} ~ {sunday_next_week.strftime('%Y.%m.%d')}"
    elif "ë‹¤ìŒ ì£¼ë§" in keyword:
        days_until_next_monday = 7 - today.weekday()
        monday_next_week = today + timedelta(days=days_until_next_monday)
        return f"ê¸°ê°„: {(monday_next_week + timedelta(days=5)).strftime('%Y.%m.%d')} ~ {(monday_next_week + timedelta(days=6)).strftime('%Y.%m.%d')}"
    if "ì´ë²ˆ ì£¼" in keyword:
        for day_name, day_index in WEEKDAY_MAP.items():
            if f"ì´ë²ˆ ì£¼ {day_name}" in keyword or f"ì´ë²ˆ ì£¼ {day_name[:-2]}" in keyword:
                days_diff = day_index - today.weekday()
                if days_diff < 0: days_diff += 7
                return f"ê¸°ê°„: {(today + timedelta(days=days_diff)).strftime('%Y.%m.%d')}"
    return ""

# ============================================
# 2. [NEW] ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ í¬ë¡¤ë§ í•¨ìˆ˜
# ============================================
def fetch_naver_blog_content(link: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë§í¬ë¥¼ ë°›ì•„ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” iframe êµ¬ì¡°ì´ë¯€ë¡œ ì‹¤ì œ URLì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        # 1. ëª¨ë°”ì¼ ë²„ì „ URLë¡œ ë³€í™˜ (iframe ì—†ì´ ë³¸ë¬¸ì´ ë°”ë¡œ ë‚˜ì˜´ & íŒŒì‹± ì‰¬ì›€)
        if "m.blog.naver.com" not in link:
            mobile_link = link.replace("blog.naver.com", "m.blog.naver.com")
        else:
            mobile_link = link
        headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"
        }
        resp = requests.get(mobile_link, headers=headers, timeout=3)
        if resp.status_code != 200:
            return ""
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ì—ë””í„° í´ë˜ìŠ¤ëª… íƒ€ê²ŸíŒ…)
        # se-main-container ë˜ëŠ” post_view í´ë˜ìŠ¤ ì•ˆì— ë³¸ë¬¸ì´ ìˆìŒ
        content = soup.find("div", class_="se-main-container")
        
        if not content:
            content = soup.find("div", id="postViewArea")
            
        if content:
            text = content.get_text(separator=" ", strip=True)
            return text[:2000] # ì• 2000ì ë°˜í™˜
            
        return ""
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return ""

# ============================================
# 3. AI ë¶„ì„ ë°ì´í„° ëª¨ë¸
# ============================================
class SearchResultItem(BaseModel):
    title: str = Field(description="ë¸”ë¡œê·¸ ì œëª©")
    link: str = Field(description="ë¸”ë¡œê·¸ ë§í¬")
    description: str = Field(description="ìš”ì•½ ë‚´ìš©")
    venue: str = Field(description="ì •í™•í•œ ì¥ì†Œëª…(ì—†ìœ¼ë©´ ë¹ˆì¹¸)")

class SearchAnalysisResult(BaseModel):
    results: List[SearchResultItem]

# ============================================
# 4. íˆ´ ì •ì˜
# ============================================
@tool
def naver_web_search(query: str, conversation_id: str) -> str:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ -> ìƒìœ„ ê²°ê³¼ í¬ë¡¤ë§ -> ì •í™•í•œ ì¥ì†Œëª… ì¶”ì¶œì˜ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.
    """
    naver_client_id = settings.NAVER_CLIENT_ID
    naver_client_secret = settings.NAVER_CLIENT_SECRET
    
    if not naver_client_id or not naver_client_secret:
        return "ì˜¤ë¥˜: configì— ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # [Step 1] ë‚ ì§œ ê³„ì‚° & ì¿¼ë¦¬ ìƒì„±
    date_info = calculate_date_range(query)
    final_query = f"{query} {date_info}" if date_info else query
    
    # ë‚ ì§œ íŒíŠ¸ê°€ ì—†ì„ ê²½ìš° ì—°ë„ ì¶”ê°€
    today = datetime.now()
    if str(today.year) not in final_query and not date_info:
         final_query = f"{today.year}ë…„ {today.month}ì›” {final_query}"

    shown_items = set(get_shown_facility_names(conversation_id)) if conversation_id else set()

    # [Step 2] ë„¤ì´ë²„ API í˜¸ì¶œ (Snippet ê²€ìƒ‰)
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {"X-Naver-Client-Id": naver_client_id, "X-Naver-Client-Secret": naver_client_secret}
    params = {"query": final_query, "display": 20, "sort": "sim"} # ì •í™•ë„ìˆœ

    try:
        if conversation_id:
            set_status(conversation_id, "ì›¹ ì •ë³´ í™•ì¸ ì¤‘..")

        response = requests.get(url, headers=headers, params=params)
        data = response.json()
        
        if not data.get('items'): return f"'{final_query}' ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        raw_items = []
        for item in data['items']:
            title = item['title'].replace("<b>", "").replace("</b>", "")
            link = item['link']
            if title in shown_items: continue
            
            # ìš”ì•½ë¬¸ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ì§€ë§Œ ì¼ë‹¨ í›„ë³´êµ° ì„ ì •ì„ ìœ„í•´ ì €ì¥
            raw_items.append({
                "title": title,
                "link": link,
                "description": item['description'].replace("<b>", "").replace("</b>", ""),
                "postdate": item.get('postdate', '')
            })

        if not raw_items: return "ìƒˆë¡œìš´ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        # [Step 3] 1ì°¨ í•„í„°ë§ (Snippet ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ 3ê°œ ì„ ì •)
        llm = get_llm()
        parser = JsonOutputParser(pydantic_object=SearchAnalysisResult)
        
        prompt_filter = PromptTemplate(
            template="""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_query}" (ë‚ ì§œíŒíŠ¸: {date_info})
ì˜¤ëŠ˜ ë‚ ì§œ: {today_date}

ì•„ë˜ ë¸”ë¡œê·¸ ëª©ë¡ ì¤‘ ê°€ì¥ ê´€ë ¨ì„± ë†’ê³  ìµœì‹  ì •ë³´ì¸ **ìƒìœ„ 3ê°œ**ë§Œ ì„ íƒí•˜ì„¸ìš”.
(ì‘ë…„ ê¸€, ê´‘ê³ , ê´€ë ¨ ì—†ëŠ” ì§€ì—­ ì œì™¸)

ëª©ë¡:
{raw_data}

ì¶œë ¥ í˜•ì‹: JSON
{format_instructions}
""",
            input_variables=["user_query", "date_info", "today_date", "raw_data"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        # API ê²°ê³¼(raw_items)ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ì „ë‹¬
        raw_text_list = [f"- ì œëª©: {i['title']}\n  ë§í¬: {i['link']}\n  ìš”ì•½: {i['description']}\n  ë‚ ì§œ: {i['postdate']}" for i in raw_items[:15]]
        
        analysis = (prompt_filter | llm | parser).invoke({
            "user_query": query,
            "date_info": date_info,
            "today_date": today.strftime("%Y-%m-%d"),
            "raw_data": "\n\n".join(raw_text_list)
        })
        
        top_3_results = analysis['results'] # ì—¬ê¸°ì„œ venueëŠ” ì•„ì§ ë¶€ì •í™•í•  ìˆ˜ ìˆìŒ

       # [Step 4] 2ì°¨ ì •ë°€ ë¶„ì„ (í¬ë¡¤ë§ + ì¥ì†Œ/ìš”ì•½ ì¬ì¶”ì¶œ)
        final_output_results = []
        
        for item in top_3_results:
            # 1. ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸°
            full_text = fetch_naver_blog_content(item['link'])
            
            if full_text:
                # 2. LLMì—ê²Œ ë‘ ê°€ì§€ ë¯¸ì…˜ ë¶€ì—¬ (ì¥ì†Œ ì°¾ê¸° + í•µì‹¬ ìš”ì•½)
                refine_prompt = f"""
                ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ì½ê³  ë‹¤ìŒ ë‘ ê°€ì§€ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´.
                
                [ë³¸ë¬¸]: {full_text[:1500]}...
                
                [ë¯¸ì…˜]
                1. venue: í–‰ì‚¬ê°€ ì—´ë¦¬ëŠ” **ê²€ìƒ‰ ê°€ëŠ¥í•œ ê±´ë¬¼ëª…/ì‹œì„¤ëª…**ë§Œ ì¶”ì¶œí•´. 
                   - [ì¤‘ìš”] 'OOOì„¼í„° 3ì¸µ', 'OOí™€' ì²˜ëŸ¼ ì¸µìˆ˜ë‚˜ í˜¸ìˆ˜ëŠ” ì œë°œ ë¹¼ì¤˜. (ì§€ë„ ê²€ìƒ‰ì— ë°©í•´ë¨)
                   - ì˜ˆ: "ë²¡ìŠ¤ì½” ì œ1ì „ì‹œì¥ 2í™€" (X) -> "ë²¡ìŠ¤ì½” ì œ1ì „ì‹œì¥" (O)
                   - ì¥ì†Œê°€ ì—†ìœ¼ë©´ 'ì¥ì†Œ ë¶ˆëª…'ì´ë¼ê³  ì ì–´.

                2. summary: í–‰ì‚¬ì˜ í•µì‹¬ ì •ë³´(ì¼ì •, ì‹œê°„, ì…ì¥ë£Œ, ê¿€íŒ ë“±)ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´.
                
                [ì¶œë ¥ ì˜ˆì‹œ]
                {{
                    "venue": "ì„±ìˆ˜ë™ ì—ìŠ¤íŒ©í† ë¦¬ Dë™",
                    "summary": "11ì›” 25ì¼ê¹Œì§€ ì§„í–‰ë˜ë©° ì…ì¥ë£ŒëŠ” ë¬´ë£Œì…ë‹ˆë‹¤. ëŒ€ê¸° ì‹œê°„ì´ ê¸°ë‹ˆ ì˜¤í”ˆëŸ°ì„ ì¶”ì²œí•©ë‹ˆë‹¤."
                }}
                """
                
                try:
                    # LLM í˜¸ì¶œ ë° JSON íŒŒì‹± (ê°„ë‹¨í•˜ê²Œ contentë§Œ ê°€ì ¸ì™€ì„œ json.loads ì‹œë„)
                    refined_result = llm.invoke(refine_prompt).content.strip()
                    # í˜¹ì‹œ ëª¨ë¥¼ ë§ˆí¬ë‹¤ìš´ ì œê±°
                    refined_result = refined_result.replace("```json", "").replace("```", "")
                    refined_data = json.loads(refined_result)
                    
                    # 3. ê²°ê³¼ ì—…ë°ì´íŠ¸
                    # ì¥ì†Œê°€ ì°¾ì•„ì¡Œìœ¼ë©´ ì—…ë°ì´íŠ¸
                    if refined_data.get("venue") and "ë¶ˆëª…" not in refined_data["venue"]:
                        item['venue'] = refined_data["venue"]
                    
                    # ìš”ì•½ ë‚´ìš©ì´ ìˆìœ¼ë©´ ê¸°ì¡´ descriptionì„ ë®ì–´ì“°ê¸°
                    if refined_data.get("summary"):
                        item['description'] = "âœ¨AIìš”ì•½: " + refined_data["summary"]
                        
                except Exception as e:
                    # ì—ëŸ¬ ë‚˜ë©´ ê·¸ëƒ¥ ì›ë˜ APIê°€ ì¤€ ì •ë³´(item['venue'], item['description']) ìœ ì§€
                    print(f"ë³¸ë¬¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            final_output_results.append(item)

        # [Step 5] ê²°ê³¼ ë°˜í™˜
        if conversation_id:
            save_data = [{"name": item['title'], "link": item['link']} for item in final_output_results]
            save_search_results(conversation_id, save_data)

        result_text = f"ğŸ” '{final_query}' ê²€ìƒ‰ ë° ì •ë°€ ë¶„ì„ ê²°ê³¼:\n\n"
        
        for idx, item in enumerate(final_output_results, 1):
            html_link = f'<a href="{item["link"]}" target="_blank">ğŸ‘‰ ë¸”ë¡œê·¸ ë³´ê¸°</a>'
            result_text += f"{idx}. **{item['title']}**\n"
            # ğŸ“ í¬ë¡¤ë§ìœ¼ë¡œ ì°¾ì•„ë‚¸ ì •í™•í•œ ì¥ì†Œ í‘œì‹œ
            result_text += f"   - ğŸ“ ì¥ì†Œ: {item['venue']}\n" 
            result_text += f"   - ğŸ“ ë‚´ìš©: {item['description']}\n"
            result_text += f"   - {html_link}\n\n"
            
        return result_text

    except Exception as e:
        return f"ê²€ìƒ‰ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
