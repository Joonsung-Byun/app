import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List
from config import settings
from models.chat_models import get_llm
from utils.conversation_memory import save_search_results, get_shown_facility_names, set_status 

# ë§˜ì¹´í˜ ì°¨ë‹¨ íšŒí”¼ë¥¼ ìœ„í•œ ì™„ì „í•œ User-Agent
USER_AGENT = "Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1"

# ============================================
# 1. ë¹„ë™ê¸° í¬ë¡¤ë§ í—¬í¼ í•¨ìˆ˜
# ============================================

async def fetch_single_cafe(session, link: str) -> str:
    """ê°œë³„ ì¹´í˜ ê¸€ì„ ë¹„ë™ê¸°ë¡œ í¬ë¡¤ë§."""
    try:
        # ëª¨ë°”ì¼ ë§í¬ë¡œ ë³€í™˜í•˜ì—¬ ë³¸ë¬¸ ì ‘ê·¼ ìš©ì´í•˜ê²Œ í•¨
        target = link.replace("cafe.naver.com", "m.cafe.naver.com")
        headers = {"User-Agent": USER_AGENT} 
        
        async with session.get(target, headers=headers, timeout=3) as resp:
            if resp.status != 200: return ""
            html = await resp.text()
            soup = BeautifulSoup(html, 'html.parser')
        
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content = soup.find("div", class_="se-main-container")
            if not content: content = soup.find("div", id="postContent")
            
            if content: return content.get_text(" ", strip=True)[:850]
            return ""
    except:
        return ""

async def fetch_cafe_urls(links: List[str]):
    """ì—¬ëŸ¬ ì¹´í˜ ê¸€ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§."""
    async with aiohttp.ClientSession() as session:
        return await asyncio.gather(*[fetch_single_cafe(session, l) for l in links])

# ============================================
# 2. AI ë¶„ì„ ë°ì´í„° ëª¨ë¸
# ============================================

class CafeItem(BaseModel):
    title: str = Field(description="ì œëª©")
    link: str = Field(description="ë§í¬")
    summary: str = Field(description="ì†”ì§ ìš”ì•½")
    sentiment: str = Field(description="ê¸ì •/ë¶€ì •/ì¤‘ë¦½")

class CafeAnalysis(BaseModel):
    results: List[CafeItem]

# ============================================
# 3. íˆ´ ì •ì˜ 
# ============================================

@tool
async def naver_cafe_search(query: str, conversation_id: str) -> str:
    """
    ë„¤ì´ë²„ ë§˜ì¹´í˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ 'ì†”ì§ í›„ê¸°', 'ì¥ë‹¨ì ', 'ì£¼ì°¨/ì›¨ì´íŒ… ê¿€íŒ'ì„ í™•ì¸í•©ë‹ˆë‹¤. (ì™„ì „ ë¹„ë™ê¸°)
    ê²€ì¦ì´ë‚˜ í‰íŒ ì¡°íšŒê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    naver_id = settings.NAVER_CLIENT_ID
    naver_secret = settings.NAVER_CLIENT_SECRET
    
    if not naver_id or not naver_secret:
        return "ì˜¤ë¥˜: ì„œë²„ ì„¤ì •(config)ì— ë„¤ì´ë²„ API í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."

    # [Step 1] ì¹´í˜ ê²€ìƒ‰ API ì„¤ì •
    url = "https://openapi.naver.com/v1/search/cafearticle.json"
    headers = {
        "X-Naver-Client-Id": naver_id, 
        "X-Naver-Client-Secret": naver_secret
    }
    params = {"query": query, "display": 10, "sort": "sim"} 
    
    try:
        if conversation_id:
            set_status(conversation_id, "ë§˜ì¹´í˜ í›„ê¸° ê²€ìƒ‰ ì¤‘...")
            
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, params=params) as resp:
        
                # API í˜¸ì¶œ ì‹¤íŒ¨ ì˜¤ë¥˜ ë°©ì§€ (resp.status ì‚¬ìš©)
                if resp.status != 200:
                    return f"ë„¤ì´ë²„ API ì˜¤ë¥˜ ë°œìƒ (ìƒíƒœì½”ë“œ: {resp.status})"

                # ì‘ë‹µ JSONì„ ë¹„ë™ê¸°ë¡œ ê°€ì ¸ì˜¤ê¸°
                data = await resp.json() 
        
        if not data.get('items'): return "ê´€ë ¨ ì¹´í˜ í›„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤."

        raw_items = []
        shown = set(get_shown_facility_names(conversation_id)) if conversation_id else set()
        
        for item in data['items']:
            title = item['title'].replace("<b>","").replace("</b>","")
            if title in shown: continue
            raw_items.append({"title": title, "link": item['link'], "desc": item['description']})

        if not raw_items: return "ìƒˆë¡œìš´ í›„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # [Step 2] LLM 1ì°¨ ì„ ë³„ (
        llm = get_llm()
        parser = JsonOutputParser(pydantic_object=CafeAnalysis)
        
        prompt = PromptTemplate(
            template="""
            ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
            ì•„ë˜ ë§˜ì¹´í˜ ê¸€ ì¤‘ **ê°€ì¥ ì†”ì§í•˜ê³  ë„ì›€ë˜ëŠ” í›„ê¸° 3ê°œ**ë¥¼ ê³¨ë¼ì£¼ì„¸ìš”.
            (ë‹¨ìˆœ í™ë³´, ì§ˆë¬¸ê¸€ ì œì™¸. 'ë‹¤ë…€ì™”ì–´ìš”' í›„ê¸° ìš°ì„ )
            
            ëª©ë¡:
            {raw_data}
            
            ì¶œë ¥ í˜•ì‹: JSON
            {format_instructions}
            """,
            input_variables=["user_query", "raw_data"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        raw_text = "\n".join([f"- {i['title']} ({i['link']}) : {i['desc']}" for i in raw_items[:10]])
        
        chain = prompt | llm | parser
        analysis = await chain.ainvoke({"user_query": query, "raw_data": raw_text})
        top_3 = analysis['results']

        # [Step 3] ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§ (await ì‚¬ìš©)
        target_links = [item['link'] for item in top_3]
        contents = await fetch_cafe_urls(target_links) 

        final_results = []
        for idx, item in enumerate(top_3):
            full_text = contents[idx]
            
            if full_text:
                refine_prompt = f"""
                ë§˜ì¹´í˜ í›„ê¸° ë³¸ë¬¸ì„ ë³´ê³  'ì—„ë§ˆë“¤ì„ ìœ„í•œ ì° ê¿€íŒ'ì„ í•œ ì¤„ë¡œ ìš”ì•½í•´ì¤˜.
                (ì˜ˆ: ì£¼ì°¨ì¥ ë§Œì°¨ ì‹œê°„, ì¤€ë¹„ë¬¼, ë¹„ì¶”ì²œ ì´ìœ  ë“±)
                
                [ë³¸ë¬¸]: {full_text}
                """
                try:
                    tip_msg = await llm.ainvoke(refine_prompt)
                    tip = tip_msg.content.strip()
                    item['summary'] = f"{item['summary']} (ğŸ’¡ {tip})"
                except: pass
            
            final_results.append(item)

        # [Step 4] ë°˜í™˜
        if conversation_id:
            save_data = [
                {
                    "name": i.get("venue") or i.get("title"),
                    "link": i.get("link"),
                    "desc": i.get("summary", "")
                }
                for i in final_results
            ]
            save_search_results(conversation_id, save_data)

        res_text = f"â˜• **'{query}' ë§˜ì¹´í˜ ì°í›„ê¸°**\n"
        for i, item in enumerate(final_results, 1):
            icon = "ğŸ‘" if item['sentiment'] == "ê¸ì •" else "ğŸ’¬"
            link = f'<a href="{item["link"]}" target="_blank">ê¸€ ë³´ê¸°</a>'
            res_text += f"\n{i}. {icon} **{item['title']}**\n"
            res_text += f"   ğŸ“ {item['summary']}\n"
            res_text += f"   ğŸ”— {link}\n"
            
        return res_text

    except Exception as e:
        return f"ì¹´í˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"
