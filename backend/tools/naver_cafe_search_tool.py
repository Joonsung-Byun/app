import requests
import json
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from langchain_core.tools import tool
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from typing import List
from config import settings
from models.chat_models import get_llm
from utils.conversation_memory import save_search_results, get_shown_facility_names
import nest_asyncio

nest_asyncio.apply()

# --- ë¹„ë™ê¸° í¬ë¡¤ë§ í•¨ìˆ˜ (ë¸”ë¡œê·¸ì™€ ë™ì¼í•˜ê²Œ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ë‚˜, ì¹´í˜ íŠ¹í™” ì²˜ë¦¬ ìœ„í•´ ë³„ë„ ì‘ì„±) ---
async def fetch_single_cafe(session, link: str) -> str:
    try:
        # ì¹´í˜ëŠ” ëª¨ë°”ì¼ ë§í¬ ë³€í™˜ì´ ë” ì¤‘ìš”í•¨
        target = link.replace("cafe.naver.com", "m.cafe.naver.com")
        headers = {"User-Agent": "Mozilla/5.0 ..."} # (User-Agent í•„ìˆ˜)
        
        async with session.get(target, headers=headers, timeout=3) as resp:
            if resp.status != 200: return ""
            html = await resp.text()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì¹´í˜ ë³¸ë¬¸ í´ë˜ìŠ¤ (se-main-container ë“±)
            content = soup.find("div", class_="se-main-container")
            if not content: content = soup.find("div", id="postContent")
            
            if content: return content.get_text(" ", strip=True)[:1500]
            return ""
    except:
        return ""

async def fetch_cafe_urls(links: List[str]):
    async with aiohttp.ClientSession() as session:
        return await asyncio.gather(*[fetch_single_cafe(session, l) for l in links])

# --- ë°ì´í„° ëª¨ë¸ ---
class CafeItem(BaseModel):
    title: str = Field(description="ì œëª©")
    link: str = Field(description="ë§í¬")
    summary: str = Field(description="ì†”ì§ ìš”ì•½")
    sentiment: str = Field(description="ê¸ì •/ë¶€ì •/ì¤‘ë¦½")

class CafeAnalysis(BaseModel):
    results: List[CafeItem]

@tool
def naver_cafe_search(query: str, conversation_id: str) -> str:
    """
    ë„¤ì´ë²„ ë§˜ì¹´í˜ë¥¼ ê²€ìƒ‰í•˜ì—¬ 'ì†”ì§ í›„ê¸°', 'ì¥ë‹¨ì ', 'ì£¼ì°¨/ì›¨ì´íŒ… ê¿€íŒ'ì„ í™•ì¸í•©ë‹ˆë‹¤.
    ê²€ì¦ì´ë‚˜ í‰íŒ ì¡°íšŒê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.
    """
    naver_id = settings.NAVER_CLIENT_ID
    naver_secret = settings.NAVER_CLIENT_SECRET
    
    # [Step 1] ì¹´í˜ ê²€ìƒ‰ API
    url = "https://openapi.naver.com/v1/search/cafearticle.json"
    headers = {"X-Naver-Client-Id": naver_id, "X-Naver-Client-Secret": naver_secret}
    params = {"query": query, "display": 15, "sort": "sim"} # ì •í™•ë„ìˆœ
    
    try:
        resp = requests.get(url, headers=headers, params=params)
        data = resp.json()
        if not data.get('items'): return "ê´€ë ¨ ì¹´í˜ í›„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤."

        raw_items = []
        shown = set(get_shown_facility_names(conversation_id)) if conversation_id else set()
        
        for item in data['items']:
            title = item['title'].replace("<b>","").replace("</b>","")
            if title in shown: continue
            raw_items.append({"title": title, "link": item['link'], "desc": item['description']})

        if not raw_items: return "ìƒˆë¡œìš´ í›„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # [Step 2] LLM 1ì°¨ ì„ ë³„
        llm = get_llm()
        parser = JsonOutputParser(pydantic_object=CafeAnalysis)
        
        prompt = PromptTemplate(
            template="""
            ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
            ì•„ë˜ ë§˜ì¹´í˜ ê¸€ ì¤‘ **ê°€ì¥ ì†”ì§í•˜ê³  ë„ì›€ë˜ëŠ” í›„ê¸° 3ê°œ**ë¥¼ ê³¨ë¼ì£¼ì„¸ìš”.
            (ë‹¨ìˆœ í™ë³´, ì§ˆë¬¸ê¸€ ì œì™¸. 'ë‹¤ë…€ì™”ì–´ìš”' í›„ê¸° ìš°ì„ )
            
            ëª©ë¡:
            {raw_data}
            
            ì¶œë ¥ í˜•ì‹: JSON
            {format_instructions}
            """,
            input_variables=["user_query", "raw_data"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        raw_text = "\n".join([f"- {i['title']} ({i['link']}) : {i['desc']}" for i in raw_items[:10]])
        analysis = (prompt | llm | parser).invoke({"user_query": query, "raw_data": raw_text})
        top_3 = analysis['results']

        # [Step 3] âš¡ ì¹´í˜ê¸€ ë¹„ë™ê¸° ë³‘ë ¬ í¬ë¡¤ë§
        target_links = [item['link'] for item in top_3]
        contents = asyncio.run(fetch_cafe_urls(target_links))

        final_results = []
        for idx, item in enumerate(top_3):
            full_text = contents[idx]
            
            # ë³¸ë¬¸ì„ ê¸ì–´ì™”ìœ¼ë©´ ë‚´ìš©ì„ ë” ë³´ê°•í•¨ (ì•ˆ ê¸í˜”ìœ¼ë©´ 1ì°¨ LLM ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            if full_text:
                refine_prompt = f"""
                ë§˜ì¹´í˜ í›„ê¸° ë³¸ë¬¸ì„ ë³´ê³  'ì—„ë§ˆë“¤ì„ ìœ„í•œ ì° ê¿€íŒ'ì„ í•œ ì¤„ë¡œ ìš”ì•½í•´ì¤˜.
                (ì˜ˆ: ì£¼ì°¨ì¥ ë§Œì°¨ ì‹œê°„, ì¤€ë¹„ë¬¼, ë¹„ì¶”ì²œ ì´ìœ  ë“±)
                
                [ë³¸ë¬¸]: {full_text}
                """
                try:
                    tip = llm.invoke(refine_prompt).content.strip()
                    item['summary'] = f"{item['summary']} (ğŸ’¡ {tip})"
                except: pass
            
            final_results.append(item)

        # [Step 4] ë°˜í™˜
        if conversation_id:
             save_data = [{"name": i['title'], "link": i['link']} for i in final_results]
             save_search_results(conversation_id, save_data)

        res_text = f"â˜• **'{query}' ë§˜ì¹´í˜ ì°í›„ê¸°**:\n\n"
        for i, item in enumerate(final_results, 1):
            icon = "ğŸ‘" if item['sentiment'] == "ê¸ì •" else "ğŸ’¬"
            link = f'<a href="{item["link"]}" target="_blank">ê¸€ ë³´ê¸°</a>'
            res_text += f"{i}. {icon} **{item['title']}**\n   ğŸ—£ï¸ {item['summary']}\n   ğŸ”— {link}\n\n"
            
        return res_text

    except Exception as e:
        return f"ì¹´í˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"